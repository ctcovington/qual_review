<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.168">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Qualifying Exam Prep Session #6: Bayesian Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="session_06_files/libs/clipboard/clipboard.min.js"></script>
<script src="session_06_files/libs/quarto-html/quarto.js"></script>
<script src="session_06_files/libs/quarto-html/popper.min.js"></script>
<script src="session_06_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="session_06_files/libs/quarto-html/anchor.min.js"></script>
<link href="session_06_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="session_06_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="session_06_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="session_06_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="session_06_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="custom.css">
</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Qualifying Exam Prep Session #6: Bayesian Methods</h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load necessary libraries</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glue)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(nimble)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Inference within the Bayesian framework is conceptually very simple, it all follows from the rules of probability! The analyst starts with a data model <span class="math inline">\(\mathcal{M} = \left\{ p(x \vert \theta) : \theta \in \Theta \right\}\)</span> for how the world generates data from parameters, as well as prior beliefs about the parameters encoded in a probability distribution <span class="math inline">\(p(\theta)\)</span>. Bayes’ Theorem provides the machinery needed to turn the analyst’s prior beliefs into <em>posterior beliefs</em>, <span class="math display">\[
    p(\theta \vert x)
    =
    \frac{p(x \vert \theta) p(\theta)}{p(x)}
    \propto
    p(x \vert \theta) p (\theta),
\]</span> where <span class="math inline">\(p(x) = \int\limits_{\theta \in \Theta} p(x \vert \theta) p (\theta) d \theta\)</span>.</p>
<p>The posterior <span class="math inline">\(p(\theta \vert x)\)</span> is typically the core element of interest in a Bayesian analysis, much like point estimates and confidence regions might be in a Frequentist analysis.</p>
<section id="terminology" class="level2">
<h2 class="anchored" data-anchor-id="terminology">Terminology</h2>
<ul>
<li><strong>Prior:</strong> <span class="math inline">\(p(\theta)\)</span>
<ul>
<li>This is traditionally taught as a representation of the analyst’s beliefs (I even described it as such above). However, there are other approaches such as <em>Objective Bayes</em> <span class="citation" data-cites="berger2024objective">(<a href="#ref-berger2024objective" role="doc-biblioref">Berger, Bernardo, and Sun 2024</a>)</span> and <em>Pragmatic Bayes</em> (this isn’t a well-defined term, but I’m referring to the philosophy advanced in <span class="citation" data-cites="gelman2013bayesian">(<a href="#ref-gelman2013bayesian" role="doc-biblioref">Gelman et al. 2013</a>)</span>) which are probably closer to how people tend to perform Bayesian inference in practice. This is where we see ideas like weakly informative priors (more associated with the pragmatic school), reference priors (more associated with the objective school), etc.</li>
</ul></li>
<li><strong>Posterior:</strong> <span class="math inline">\(p(\theta \vert x)\)</span>
<ul>
<li>Essentially all information conveyed in a Bayesian analysis is based on the posterior.</li>
</ul></li>
<li><strong>Marginal Likelihood:</strong> <span class="math inline">\(p(x)\)</span>
<ul>
<li>Often difficult to calculate, typically used just for normalization (ensuring the posterior is a valid probability distribution)</li>
<li>Sometimes written <span class="math inline">\(p(x \vert \mathcal{M}) = \int\limits_{\theta \in \Theta} p(x \vert \theta, \mathcal{M}) p(\theta \vert \mathcal{M}) d \theta\)</span>, acknowledging that everything we do in Bayesian analysis is really conditional on the analyst’s choice of the data model <span class="math inline">\(\mathcal{M}\)</span>.
<ul>
<li>When written conditional on <span class="math inline">\(\mathcal{M}\)</span>, it becomes more clear conceptually that this can be used for model comparison.</li>
</ul></li>
</ul></li>
<li><strong>Posterior Predictive</strong> <span class="math inline">\(p(x^{\textrm{rep}} \vert x) = \int\limits_{\theta \in \Theta} p(x^{\textrm{rep}} \vert \theta) p(\theta \vert x) d \theta\)</span>
<ul>
<li>Makes model “generative”</li>
<li>Useful for model checking/criticism, enables the <em>posterior predictive check</em> <span class="citation" data-cites="gelman2013bayesian">(<a href="#ref-gelman2013bayesian" role="doc-biblioref">Gelman et al. 2013</a>)</span>. If observed <span class="math inline">\(x\)</span> is inconsistent with <span class="math inline">\(p(x^{\textrm{rep}} \vert x)\)</span>, this suggests a problem with the prior or data model.</li>
</ul></li>
</ul>
</section>
</section>
<section id="conjugate-priors" class="level1">
<h1>Conjugate Priors</h1>
<p>Suppose we have a data model <span class="math inline">\(\mathcal{M} = \left\{ p(x \vert \theta) : \theta \in \Theta \right\}\)</span> and a set <span class="math inline">\(\mathcal{X}\)</span> of data sets which are possible under <span class="math inline">\(\mathcal{M}\)</span>. We call <span class="math inline">\(\mathcal{P} = \left\{ p_{\alpha}(\theta) : \alpha \in H \right\}\)</span> a <em>conjugate prior</em> if, <span class="math display">\[
\forall \alpha \in H, \forall x \in \mathcal{X}:
\exists \alpha' \in H \text{ s.t. } p(\theta \vert x) = p_{\alpha'}(\theta).
\]</span></p>
<p>Using a conjugate prior for a particular problem comes with two huge advantages. The first is a computational/practical advantage; for every conjugate prior I’m aware of, we have a known map from <span class="math inline">\(x\)</span> to <span class="math inline">\(\alpha'\)</span>; in other words, we have a closed-form solution for the posterior. The second is a conceptual advantage; if we interpret the prior as representing belief, it seems nice that when we update our beliefs they stay within the same family of distributions.</p>
<section id="exercise-1-conjugate-priors" class="level3">
<h3 class="anchored" data-anchor-id="exercise-1-conjugate-priors">Exercise #1: Conjugate Priors</h3>
<p>Suppose we have <span class="math inline">\(X_{1:n} \stackrel{\textrm{i.i.d.}}{\sim} \textrm{Exp}(\theta)\)</span> such that <span class="math inline">\(p(x_i \vert \theta) = \theta \exp(-\theta x_i) \mathbf{1}(x &gt; 0)\)</span>.</p>
<p>The Gamma<span class="math inline">\((\alpha,\beta)\)</span> distribution (with its shape/rate parameterization) is given by <span class="math display">\[
\textrm{Gamma}(\theta \vert \alpha, \beta)
=
\frac{\beta^{\alpha}}{\Gamma(\alpha)}
\theta^{\alpha-1}
\exp(-\beta \theta)
\mathbf{1}(\theta &gt; 0).
\]</span></p>
<p>Show that the Gamma distribution is a conjugate prior for the Exponential data model, and find the posterior in terms of <span class="math inline">\(x_{1:n}\)</span>.</p>
<details>
<summary style="font-weight: bold; cursor: pointer;">
Solution
</summary>
<p><br> <span class="math display">\[
\begin{align*}
    p(\theta \vert x_{1:n})
        &amp;\propto p(x_{1:n} \vert \theta) p(\theta) \\
        &amp;= \left( \prod_{i=1}^{n} p(x_i \vert \theta) \right) p(\theta) &amp;&amp; \text{ ($x_i \sim $ i.i.d.) } \\
        &amp;= \left( \prod_{i=1}^{n} \theta \exp(- \theta x_i) \mathbf{1}(x_i &gt; 0) \right) \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha-1} \exp(-\beta \theta) \mathbf{1}(\theta &gt; 0) \\
        &amp;= \theta^n \exp(-\theta \sum_{i=1}^{n}x_i \mathbf{1}(x_i &gt; 0) ) \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{\alpha-1} \exp(-\beta \theta) \mathbf{1}(\theta &gt; 0) \\
        &amp;= \frac{\beta^{\alpha}}{\Gamma(\alpha)} \theta^{n + \alpha - 1}
           \exp \left( - \theta (\beta + \sum_{i=1}^{n} x_i \mathbf{1}(x_i &gt; 0)) \right) \\
        &amp;\propto \frac{\beta'^{\alpha'}}{\Gamma(\alpha')} \theta^{\alpha'-1} \exp \left( -\theta\beta' \right) \\
        &amp;= \textrm{Gamma}(\alpha', \beta'),
\end{align*}
\]</span> where <span class="math inline">\(\alpha' = \alpha + n\)</span> and <span class="math inline">\(\beta' = \beta + \sum_{i=1}^{n}x_i\)</span>.</p>
</details>
</section>
</section>
<section id="markov-chain-monte-carlo" class="level1">
<h1>(Markov Chain) Monte Carlo</h1>
<section id="monte-carlo-approximation" class="level2">
<h2 class="anchored" data-anchor-id="monte-carlo-approximation">Monte Carlo Approximation</h2>
<p>In Bayesian statistics, many inferential tasks involve computing some sort of expectation <span class="math display">\[
\mathbb{E} h(Y)
=
\int h(y) p(y) dy.
\]</span></p>
<p>We may not know <span class="math inline">\(\mathbb{E} h(Y)\)</span> analytically, but as long as we can sample from <span class="math inline">\(p(y)\)</span> we should be able to (1) estimate <span class="math inline">\(\mathbb{E} h(Y)\)</span> well and (2) characterize the uncertainty in our estimate.</p>
<p>To do so we draw on our knowledge from Prob I. Let <span class="math inline">\((Y_1, \dots, Y_m)\)</span> be draws from <span class="math inline">\(p(y).\)</span> Then <span class="math display">\[
\bar{h}_m
=
m^{-1} \sum\limits_{j=1}^{m} h(Y_j)
\]</span> has a number of nice properties which are implied by the strong law of large numbers and central limit theorem. In particular, provided that <span class="math inline">\(\mathbb{E} \left[ h(Y)^2 \right] &lt; \infty\)</span>, we have <span class="math display">\[
\begin{align*}
    \bar{h}_m &amp;\stackrel{\textrm{a.s.}}{\rightarrow} \mathbb{E} h(Y) \\
    \mathbb{E}[\bar{h}_m] &amp;= \mathbb{E} h(Y) \\
    \textrm{Var} \left( \bar{h}_m \right) &amp;= m^{-1} \int_{\mathcal{Y}} ( h(y) - \mathbb{E}[h(Y)] )^2 f(y)dy \\
        &amp;\approx m^{-1} \sum\limits_{j=1}^{m} ( h(Y_j) - \bar{h}_m )^2 := \hat{\sigma}^2_m \\
    \bar{h}_m &amp;\rightarrow N \left( \mathbb{E} h(Y), m^{-1} \textrm{Var} [h(Y)] \right) \\
    &amp;\approx N \left( \mathbb{E} h(Y), m^{-1} \hat{\sigma}^2_m \right)
\end{align*}
\]</span></p>
<p>So, <span class="math inline">\(\bar{h}_m \pm 1.96 \frac{\hat{\sigma}_m}{\sqrt{m}}\)</span> is an asymptotic 95% confidence interval for <span class="math inline">\(\mathbb{E} h(Y)\)</span>.</p>
<section id="exercise-2-monte-carlo-approximation-to-the-posterior-predictive" class="level3">
<h3 class="anchored" data-anchor-id="exercise-2-monte-carlo-approximation-to-the-posterior-predictive">Exercise #2: Monte Carlo approximation to the posterior predictive</h3>
<p>Suppose we have a similar setup as Exercise #1, with <span class="math inline">\(X_{1:n} \stackrel{\textrm{i.i.d.}}{\sim} \textrm{Exp}(\theta)\)</span> and a prior of <span class="math inline">\(\theta \sim \textrm{Gamma}(\alpha = 2, \beta = 2)\)</span>.</p>
<p>Now suppose we observe the following <code>x</code> vector:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> x <span class="sc">/</span> <span class="fu">sum</span>(x) <span class="sc">*</span> <span class="dv">10</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.4814179 0.6747317 1.0386926 1.6467542 0.3656879 1.6289521 1.7128767
 [8] 1.1981526 1.1407039 0.1120303</code></pre>
</div>
</div>
<p><strong>Part (a)</strong></p>
<p>Use a Monte Carlo algorithm to approximate the posterior predictive density <span class="math inline">\(p(x_{n+1} \vert x_{1:n})\)</span> at the point <span class="math inline">\(x_{n+1} = 0\)</span>.</p>
<details>
<summary style="font-weight: bold; cursor: pointer;">
Solution
</summary>
<p><br></p>
<p>Recall from Exercise #1 that the Gamma is a conjugate prior for the Exponential<span class="math inline">\((\theta)\)</span> data model, so we know that the posterior on <span class="math inline">\(\theta\)</span> is Gamma<span class="math inline">\((\alpha', \beta')\)</span>, where <span class="math inline">\(\alpha' = \alpha + n\)</span> and <span class="math inline">\(\beta' = \beta + \sum\limits_{i=1}^{n} x_i\)</span>. From here, we can implement our Monte Carlo algorithm, repeatedly sampling <span class="math inline">\(\theta\)</span> from the Gamma<span class="math inline">\((\alpha', \beta')\)</span> posterior and evaluating the exponential density at 0 for each posterior <span class="math inline">\(\theta\)</span> value.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set up parameters</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># use conjugacy (Exercise 1) to get alpha', beta' analytically</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>alpha_p <span class="ot">&lt;-</span> alpha <span class="sc">+</span> n</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>beta_p <span class="ot">&lt;-</span> beta <span class="sc">+</span> <span class="fu">sum</span>(x)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co"># repeatedly sample from posterior, then evaluate posterior predictive density at 0</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>n_draws <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">3</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>pp_draws <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_draws)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_draws) {</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># draw from posterior on theta</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    posterior_theta <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">1</span>, <span class="at">shape =</span> alpha_p, <span class="at">rate =</span> beta_p)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate density at x = 0 given posterior theta</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    pp_draws[i] <span class="ot">&lt;-</span> <span class="fu">dexp</span>(<span class="dv">0</span>, <span class="at">rate =</span> posterior_theta)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># plot histogram of posterior predictive</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(pp_draws), <span class="fu">aes</span>(<span class="at">x =</span> pp_draws)) <span class="sc">+</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">"steelblue"</span>, <span class="at">color =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Monte Carlo draws"</span>,</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"density"</span>,</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"frequency"</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="session_06_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get approximate posterior predictive at 0</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>a <span class="ot">&lt;-</span> <span class="fu">mean</span>(pp_draws)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">"Monte Carlo approximation: {a}"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Monte Carlo approximation: 0.987824221970336</code></pre>
</div>
</div>
</details>
<p><strong>Part (b)</strong></p>
<p>Empirically verify the claim that <span class="math inline">\(\bar{h}_m\)</span> is asymptotically normal. (A very rough visual verification will do).</p>
<details>
<summary style="font-weight: bold; cursor: pointer;">
Solution
</summary>
<p><br></p>
<p>We run the same procedure as in part (a), but over a large number of repeated simulations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n_sims <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="dv">4</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>ests <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_sims)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sims) {</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    pp_draws <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_draws)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_draws) {</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="co"># draw from posterior on theta</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        posterior_theta <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(<span class="dv">1</span>, <span class="at">shape =</span> alpha_p, <span class="at">rate =</span> beta_p)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="co"># evaluate density at x = 0 given posterior theta</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        pp_draws[i] <span class="ot">&lt;-</span> <span class="fu">dexp</span>(<span class="dv">0</span>, <span class="at">rate =</span> posterior_theta)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    ests[j] <span class="ot">&lt;-</span> <span class="fu">mean</span>(pp_draws)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="fu">data.frame</span>(ests), <span class="fu">aes</span>(<span class="at">x =</span> ests)) <span class="sc">+</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">"steelblue"</span>, <span class="at">color =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="fu">expression</span>(<span class="st">"Asymptotic distribution of"</span> <span class="sc">~</span> <span class="fu">bar</span>(h)[m]),</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(<span class="fu">bar</span>(h)[m]),</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"frequency"</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="session_06_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</details>
<p>One can do better than naïve Monte Carlo sampling with more advanced approaches, such as Importance Sampling.</p>
<p>Suppose, as above, that we have continuous random variables <span class="math inline">\((Y_1, \dots, Y_n)\)</span> from a distribution with density <span class="math inline">\(p(y)\)</span>. Define a density <span class="math inline">\(q\)</span> on the same space where <span class="math inline">\(p(y) &gt; 0 \implies q(y) &gt; 0\)</span>.</p>
<p>We don’t assume that we know <span class="math inline">\(p\)</span> or <span class="math inline">\(q\)</span> directly, only that we know them each up to their own normalizing constant; we’ll call the versions we know <span class="math inline">\(\tilde{p}, \tilde{q}\)</span> and define <span class="math display">\[
\tilde{w}(y)
=
\begin{cases}
    \frac{\tilde{p}(y)}{\tilde{q}(y)}, &amp;\text{ if } \tilde{q}(y) &gt; 0 \\
    0 &amp;\text{ if } \tilde{q}(y) = 0.
\end{cases}
\]</span></p>
<p>Then an importance sampling-based approximation to <span class="math inline">\(\mathbb{E} h(Y)\)</span> looks like <span class="math display">\[
\begin{align*}
    \mathbb{E} h(Y)
        &amp;= \sum\limits_{i=1}^{n} \left( \frac{\tilde{w}(Z_i)}{ \sum\limits_{j=1}^{n} \tilde{w}(Z_j) } \right)
\end{align*},
\]</span> where <span class="math inline">\(Z_{1:n} \stackrel{\textrm{i.i.d.}}{\sim} q\)</span>. That is, we need to be able to sample from <span class="math inline">\(q\)</span>, but we need only know the actual density up to a normalizing constant.</p>
<p>To minimize approximation error, we want to choose a <span class="math inline">\(q\)</span> such that <span class="math inline">\(q(x) \approx h(x) p (x)\)</span>, though in general we err on the side of choosing a more diffuse (spread out) <span class="math inline">\(q\)</span>.</p>
</section>
</section>
<section id="markov-chain-monte-carlo-1" class="level2">
<h2 class="anchored" data-anchor-id="markov-chain-monte-carlo-1">Markov Chain Monte Carlo</h2>
<p>There are cases where we can’t sample directly from <span class="math inline">\(p\)</span>, and it seems difficult to choose a good <span class="math inline">\(q\)</span> – this happens quite regularly in high-dimensional models.</p>
<p>The most common way to construct an MCMC algorithm is with something called the <em>Metropolis-Hastings algorithm</em>. Suppose we want to sample from <span class="math inline">\(p(x)\)</span> and for arbitrary <span class="math inline">\(x'\)</span> call <span class="math inline">\(q(x \vert x')\)</span> the proposal distribution (from which we can sample). For every <span class="math inline">\((x,x')\)</span> pair we define the acceptance ratio <span class="math display">\[
\alpha(x', x)
=
\frac{p(x') q(x \vert x')}{p(x) q(x' \vert x)}.
\]</span></p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Metropolis-Hastings
</div>
</div>
<div class="callout-body-container callout-body">
<p>Initialize <span class="math inline">\(x_0\)</span> and draw <span class="math inline">\(T\)</span> samples as follows. For each <span class="math inline">\(t \in \{1,\dots,T\}\)</span>:</p>
<p><strong>Step (1):</strong> Sample <span class="math inline">\(x \sim q(x \vert x_{t-1})\)</span></p>
<p><strong>Step (2):</strong> Sample <span class="math inline">\(u \sim \textrm{Uniform}(0,1)\)</span></p>
<p><strong>Step (3):</strong> If <span class="math inline">\(u &lt; \alpha(x_{t-1}, x)\)</span> then set <span class="math inline">\(x_t = x\)</span>. Otherwise, set <span class="math inline">\(x_t = x_{t-1}\)</span></p>
</div>
</div>
<p>MCMC is an incredibly general class of tools for sampling in these settings. A key difference between MC and MCMC algorithms is that the samples we get from MCMC are correlated. In practice, we often perform <em>thinning</em> (actually using only every <span class="math inline">\(k^{th}\)</span> MCMC sample) to reduce the correlation between our samples.</p>
<p>One of the most popular versions of Metropolis-Hasting is called the <em>Gibbs sampler</em>. Suppose we have a <span class="math inline">\(D\)</span>-dimensional parameter vector <span class="math inline">\(\theta = (\theta_1, \dots, \theta_D)\)</span> and want to sample from the posterior <span class="math inline">\(p(\theta \vert x)\)</span>, but it’s too complex to sample directly.</p>
<p>The Gibbs sampler iteratively samples from <span class="math inline">\(p(\theta_d \vert x, \theta_{-d})\)</span> for each <span class="math inline">\(d \in \{1,\dots,D\}\)</span>, where <span class="math inline">\(\theta_{-d}\)</span> is the full <span class="math inline">\(\theta\)</span> without its <span class="math inline">\(d^{th}\)</span> component. As we iterate through the elements of <span class="math inline">\(\theta\)</span>, we condition on the updated values. That is,</p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Gibbs Sampling
</div>
</div>
<div class="callout-body-container callout-body">
<p>Initialize <span class="math inline">\(\theta^{(0)} \in \mathbb{R}^D\)</span>. For each <span class="math inline">\(t \in \{0,\dots,T-1\}\)</span>:</p>
<p><span class="math display">\[
\begin{align*}
    \theta_1^{(t+1)}
        &amp;:= \theta_1^* \sim p \left( \theta_1^{(t+1)} \vert x, \theta_{-1}^{(t)} \right) \\
    \theta_2^{(t+1)}
        &amp;:= \theta_2^* \sim p \left( \theta_2^{(t+1)} \vert x, \theta_1^{(t+1)}, \theta_{-(1:2)}^{(t)} \right) \\
        &amp;\vdots \\
    \theta_d^{(t+1)}
        &amp;:= \theta_d^* \sim p \left( \theta_d^{(t+1)} \vert x, \theta_{1:(d-1)}^{(t+1)}, \theta_{-(d+1:D)}^{(t)} \right) \\
        &amp;\vdots \\
    \theta_D^{(t+1)}
        &amp;=: \theta_D^* \sim p \left( \theta_D^{(t+1)} \vert x, \theta_{1:(D-1)}^{(t+1)} \right).
\end{align*}
\]</span></p>
</div>
</div>
<section id="exercise-3-gibbs-sampling-as-metropolis-hastings" class="level3">
<h3 class="anchored" data-anchor-id="exercise-3-gibbs-sampling-as-metropolis-hastings">Exercise #3: Gibbs Sampling as Metropolis-Hastings</h3>
<p>Each time we update a component of <span class="math inline">\(\theta\)</span>, we are performing a Metropolis-Hastings move. Prove that this is the case. It is sufficient to state, in the Gibbs sampling context, (i) what is <span class="math inline">\(p\)</span>, (ii) what is <span class="math inline">\(q\)</span>, and (iii) what is <span class="math inline">\(\alpha\)</span>.</p>
<details>
<summary style="font-weight: bold; cursor: pointer;">
Solution
</summary>
<p><br></p>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This solution comes almost verbatim from Gregory Gundersen’s blog, which I highly recommend in general for anyone looking to learn more about fundamentals of Bayesian statistics. <span class="citation" data-cites="gundersen2020MH">(<a href="#ref-gundersen2020MH" role="doc-biblioref">Gundersen 2020</a>)</span></p>
</div>
</div>
<p>In this case our target distribution <span class="math inline">\(p\)</span> is the full posterior <span class="math inline">\(p(\theta \vert x)\)</span> and, at any given step, our proposal <span class="math inline">\(q\)</span> is of the form <span class="math inline">\(p \left( \theta_d^{(t+1)} \vert x, \theta_{1:(d-1)}^{(t+1)}, \theta_{-(d+1:D)}^{(t)} \right)\)</span>. When manipulating <span class="math inline">\(p,q\)</span> below, we will elide the superscript notation tracking the exact iteration of the algorithm.</p>
<p>It will be important for us to note that <span class="math display">\[
p(\theta \vert x)
=
p( \theta_d, \theta_{-d} \vert x )
=
p(\theta_d \vert x, \theta_{-d}) p(\theta_{-d} \vert x).
\]</span></p>
<p>In general, we know that <span class="math inline">\(\alpha(\theta^*, \theta)=\frac{p(\theta) q(\theta^* \vert \theta)}{p(\theta^*) q(\theta \vert \theta^*)}\)</span>, where <span class="math inline">\(p,q\)</span> are the target and proposal described above. In our Gibbs sampling context, this becomes <span class="math display">\[
\begin{align*}
    \alpha(\theta^*, \theta)
        &amp;=\frac{p(\theta^* \vert x) p(\theta_d \vert x,\theta_{-d})}
               {p(\theta \vert x) p(\theta^*_d \vert x,\theta^*_{-d})} \\
        &amp;= \frac{
    \textcolor{blue}{p(\theta_d^\star \vert x, \theta^\star_{-d})}
    \textcolor{red}{p(\theta_{-d}^\star \vert x)}
    \textcolor{olive}{p(\theta_d \vert x, \theta_{-d})}
}{
    \textcolor{olive}{p(\theta_d \vert x, \theta_{-d})}
    \textcolor{red}{p(\theta_{-d} \vert x)}
    \textcolor{blue}{p(\theta_d^\star \vert x, \theta^\star_{-d})}
} \\
    &amp;= 1.
\end{align*}
\]</span></p>
<p>So we have <span class="math inline">\(\alpha(\theta^*, \theta) = 1\)</span>. In other words, Gibbs sampling uses Metropolis-Hastings moves where the acceptance probability is 1.</p>
</details>
</section>
</section>
</section>
<section id="extra-exercises" class="level1">
<h1>Extra Exercises</h1>
<section id="exercise-4-neyman-scott-paradox" class="level2">
<h2 class="anchored" data-anchor-id="exercise-4-neyman-scott-paradox">Exercise #4: Neyman-Scott “Paradox”</h2>
<p>Consider pairs of random variables <span class="math inline">\((X_i, Y_i)\)</span> for <span class="math inline">\(i \in \{1,\dots,n\}\)</span> where <span class="math display">\[
X_i \stackrel{i.i.d.}{\sim} N(\mu_i, \sigma^2)
\quad
Y_i \stackrel{i.i.d.}{\sim} N(\mu_i, \sigma^2).
\]</span></p>
<p><strong>Part (a)</strong> Find the MLE for <span class="math inline">\(\sigma^2\)</span>. What do you think of the MLE as an estimator of <span class="math inline">\(\sigma^2\)</span>?</p>
<details>
<summary style="font-weight: bold; cursor: pointer;">
Solution
</summary>
<p><br></p>
<p>We begin by writing the likelihood as <span class="math display">\[
\ell( \mu_i, \sigma^2 \vert x_i,y_i )
=
p( x_i,y_i \vert \mu_i, \sigma^2 )
=
(2 \pi \sigma^2)^{-n}
\exp \left( -\frac{1}{2 \sigma^2} \sum\limits_{i=1}^{n} \left[ (x_i-\mu_i)^2 + (y_i-\mu_i)^2 \right] \right).
\]</span></p>
<p>The partial derivatives of the log-likelihood are <span class="math display">\[
\begin{align*}
\frac{\partial \ell}{\partial \mu_i}
    &amp;= \frac{x_i + y_i - 2 \mu_i}{\sigma^2} \\
\frac{\partial \ell}{\partial \sigma^2}
    &amp;= -\frac{n}{\sigma^2} + \frac{1}{2 \sigma^2} \sum\limits_{i=1}^{n} \left[ (x_i-\mu_i)^2 + (y_i - \mu_i)^2 \right].
\end{align*}
\]</span></p>
<p>We set these both to 0 and solve for <span class="math inline">\(\hat{\mu}_i, \hat{\sigma}^2\)</span>, which yields <span class="math display">\[
\begin{align*}
    \hat{\mu}_i
        &amp;= \frac{1}{2} (x_i + y_i) \\
    \hat{\sigma}^2
        &amp;= \frac{1}{4n} \sum\limits_{i=1}^{n} (x_i-y_i)^2.
\end{align*}
\]</span></p>
<p><span class="math inline">\(\hat{\sigma}^2\)</span> is not a very good estimator of <span class="math inline">\(\sigma^2\)</span>. Notably, <span class="math inline">\(\mathbb{E} \left[ \hat{\sigma}^2 \right] = \frac{1}{4} \mathbb{E} \left[ (x_i-y_i)^2 \right] = \frac{\sigma^2}{2}\)</span>, because <span class="math inline">\(X_i-Y_i \stackrel{i.i.d.}{\sim} N(\mu=0, 2\sigma^2)\)</span>. That is, the expected value of the MLE is <span class="math inline">\(\frac{1}{2}\)</span> the parameter we’re trying to estimate, even asymptotically.</p>
</details>
<p><strong>Part (b)</strong></p>
<p>Try implementing a Bayesian model for this problem. Empirically, do you encounter the same issue as you found with the MLE? Use the following code to generate the data.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>mu_true <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> <span class="dv">2</span>, <span class="fu">rep</span>(mu_true, <span class="at">each =</span> <span class="dv">2</span>), sigma_true), <span class="at">nrow =</span> n, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<details>
<summary style="font-weight: bold; cursor: pointer;">
Solution
</summary>
<p><br></p>
<p>We begin by specifying the model. Let <span class="math inline">\(TN(m,s^2,a,b)\)</span> be a <span class="math inline">\(N(m,s^2)\)</span> distribution truncated to <span class="math inline">\((a,b)\)</span>. The likelihood for <span class="math inline">\(X_i,Y_i\)</span> is given by the problem setup, the only other components we absolutely need are priors on our parameters <span class="math inline">\(\mu_i, \sigma^2\)</span>. Because <span class="math inline">\(\mu_i\)</span> differs by <span class="math inline">\(i\)</span>, we choose to put priors on the parameters on our prior for <span class="math inline">\(\mu_i\)</span> (we call these <em>hyperpriors</em>).<br>
One advantage of this, other than further expressing our uncertainty in the underlying process, is that we could get a posterior both on any individual <span class="math inline">\(\mu_i\)</span>, but also on their global mean <span class="math inline">\(\mu_0\)</span>. <span class="math display">\[
\begin{align*}
    X_i &amp;\sim N(\mu_i, \sigma^2) \\
    Y_i &amp;\sim N(\mu_i, \sigma^2) \\
    \mu_i &amp;\sim N(\mu_0, \tau^2) \\
    \sigma &amp;\sim TN(0, 10^2, 0, \infty) \\
    \mu_0 &amp;\sim N(0, 10^2) \\
    \tau &amp;\sim TN(0, 10^2, 0, \infty).
\end{align*}
\]</span></p>
<p>Next we implement the model in nimble.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>model_spec <span class="ot">&lt;-</span> <span class="fu">nimbleCode</span>({</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>  mu0 <span class="sc">~</span> <span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">10</span>) <span class="co"># weakly informative prior on mu_0</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>  tau <span class="sc">~</span> <span class="fu">T</span>(<span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">10</span>), <span class="dv">0</span>, )  <span class="co"># half-normal prior on tau</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>  sigma <span class="sc">~</span> <span class="fu">T</span>(<span class="fu">dnorm</span>(<span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">10</span>), <span class="dv">0</span>, ) <span class="co"># half-normal prior on sigma</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>) {</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>      X[i, j] <span class="sc">~</span> <span class="fu">dnorm</span>(mu[i], <span class="at">sd =</span> sigma) <span class="co"># model for X[i, 1:2] = (x_i, y_i)</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    mu[i] <span class="sc">~</span> <span class="fu">dnorm</span>(mu0, <span class="at">sd =</span> tau) <span class="co"># prior on mu_i</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Constants, data, and initial values</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>data_list <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">X =</span> X)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>constants <span class="ot">&lt;-</span> <span class="fu">list</span>(<span class="at">n =</span> n)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>inits <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu =</span> <span class="fu">rowMeans</span>(X),</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>  <span class="at">mu0 =</span> <span class="fu">mean</span>(<span class="fu">rowMeans</span>(X)),</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>  <span class="at">tau =</span> <span class="dv">1</span>,</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>  <span class="at">sigma =</span> <span class="dv">1</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Build and compile model</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">nimbleModel</span>(model_spec, <span class="at">data =</span> data_list, <span class="at">inits =</span> inits, <span class="at">constants =</span> constants)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>cModel <span class="ot">&lt;-</span> <span class="fu">compileNimble</span>(model)</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Configure and build MCMC</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>conf <span class="ot">&lt;-</span> <span class="fu">configureMCMC</span>(model, <span class="at">monitors =</span> <span class="fu">c</span>(<span class="st">'mu0'</span>, <span class="st">'tau'</span>, <span class="st">'sigma'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>===== Monitors =====
thin = 1: mu0, sigma, tau
===== Samplers =====
RW sampler (2)
  - tau
  - sigma
conjugate sampler (1001)
  - mu0
  - mu[]  (1000 elements)</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>mcmc <span class="ot">&lt;-</span> <span class="fu">buildMCMC</span>(conf)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>cMCMC <span class="ot">&lt;-</span> <span class="fu">compileNimble</span>(mcmc, <span class="at">project =</span> model)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Run MCMC</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">runMCMC</span>(cMCMC, <span class="at">niter =</span> <span class="dv">10000</span>, <span class="at">nburnin =</span> <span class="dv">2000</span>, <span class="at">thin =</span> <span class="dv">5</span>, <span class="at">summary =</span> <span class="cn">TRUE</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>|-------------|-------------|-------------|-------------|
|-------------------------------------------------------|</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># get summary statistics for posterior</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(samples<span class="sc">$</span>summary)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             Mean      Median    St.Dev.  95%CI_low  95%CI_upp
mu0   -0.01296328 -0.01417987 0.05671477 -0.1241768 0.09765024
sigma  2.06521810  2.06356749 0.04663321  1.9791568 2.16365277
tau    1.00375123  1.00912569 0.08681287  0.8154659 1.15982013</code></pre>
</div>
</div>
<p>We see from our posterior summary that our posterior on <span class="math inline">\(\sigma\)</span> is centered close to the true sigma value of 2, certainly better than the value of <span class="math inline">\(\sqrt{2}\)</span> that we would get from the MLE.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot posterior for sigma</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>posterior_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(samples<span class="sc">$</span>samples)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(posterior_df, <span class="fu">aes</span>(<span class="at">x =</span> sigma)) <span class="sc">+</span> </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_histogram</span>(<span class="at">fill =</span> <span class="st">"steelblue"</span>, <span class="at">color =</span> <span class="st">"white"</span>, <span class="at">bins =</span> <span class="dv">100</span>) <span class="sc">+</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">title =</span> <span class="fu">expression</span>(<span class="st">"Posterior distribution of"</span> <span class="sc">~</span> sigma),</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">x =</span> <span class="fu">expression</span>(sigma),</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"frequency"</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="session_06_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</details>
<!-- {{< include model_checking.qmd >}} -->
</section>
</section>
<section id="references" class="level1 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-berger2024objective" class="csl-entry" role="doc-biblioentry">
Berger, James O, José-Miguel Bernardo, and Dongchu Sun. 2024. <em>Objective Bayesian Inference</em>. World Scientific.
</div>
<div id="ref-gelman2013bayesian" class="csl-entry" role="doc-biblioentry">
Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. <em>Bayesian Data Analysis</em>. CRC Press.
</div>
<div id="ref-gundersen2020MH" class="csl-entry" role="doc-biblioentry">
Gundersen, Gregory. 2020. <span>“Gibbs Sampling Is a Special Case of Metropolis–Hastings.”</span> <a href="https://gregorygundersen.com/blog/2020/02/23/gibbs-sampling/">https://gregorygundersen.com/blog/2020/02/23/gibbs-sampling/</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>